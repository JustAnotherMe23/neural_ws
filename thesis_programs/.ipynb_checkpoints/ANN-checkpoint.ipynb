{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This is a practice Artificial Neural Network\n",
    "# The problem being solved is based off of a model bank with fake data\n",
    "# The bank has customers that have left for whatever reason\n",
    "# The goal is to find why these customers have left using information such as account balance and gender\n",
    "# The last column of the data states whether or not the customer has left the bank\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np #Math operations library\n",
    "import matplotlib.pyplot as plt #Visualization library\n",
    "import pandas as pd #Matrix handler\n",
    "\n",
    "import keras # Brings in tensorflow with it\n",
    "from keras.models import Sequential # Used for initialization of ANN\n",
    "from keras.layers import Dense, Dropout# adds layers to ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier # ability to turn network into a function definition\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler #Methods to change categorical strings to numbers and scaling ability\n",
    "\n",
    "parameters = {'batch_size': 16, # 50 epochs, 264 nodes, 3 hidden layers\n",
    "              'epochs': 100,\n",
    "              'learning_rate': 0.00001,\n",
    "              'nodes': 264,\n",
    "              'hidden_layers': 3,\n",
    "              'dropout': 0.5\n",
    "             } # Creates list of parameters to test to find most successful one\n",
    "parameters['optimizer'] = Adam(parameters['learning_rate'])\n",
    "\n",
    "class data_handler:\n",
    "    def __init__(self, pkl_location, sc):\n",
    "        data = pd.read_pickle(pkl_location)\n",
    "        data = data.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "        x = data[list(range(0, 264))]# max 264 x columns\n",
    "        x = sc.fit_transform(x)\n",
    "        self.x = pd.DataFrame(x)\n",
    "        \n",
    "        self.y = data[['Rest', 'Emote', 'Solve']]\n",
    "        del data\n",
    "        \n",
    "        self.tenth = math.floor(self.x.shape[0] / 10)\n",
    "        \n",
    "    def get_cross_val(self, iteration):\n",
    "        if iteration > 10:\n",
    "            raise ValueError('Crossval Iteration Exceeds 10')\n",
    "        \n",
    "        iteration = iteration - 1\n",
    "        train_array = list(range(0, iteration*self.tenth)) + list(range((iteration+1)*self.tenth, self.x.shape[0]))\n",
    "        test_array = list(range(iteration*self.tenth, (iteration+1)*self.tenth))\n",
    "        \n",
    "        x_train = self.x.iloc[train_array].values\n",
    "        y_train = self.y.iloc[train_array].values\n",
    "        x_test = self.x.iloc[test_array].values\n",
    "        y_test = self.y.iloc[test_array].values\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "def build_classifier(parameters):\n",
    "    classifier = Sequential() # This is the ANN object\n",
    "    classifier.add(Dense(input_dim=264, units=parameters['nodes'], kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dropout(rate=parameters['dropout']))\n",
    "    \n",
    "    for layer in range(0, parameters['hidden_layers']):\n",
    "        classifier.add(Dense(units=parameters['nodes'], kernel_initializer='uniform', activation='relu')) #Creates first hidden layer\n",
    "        classifier.add(Dropout(rate=parameters['dropout']))\n",
    "        \n",
    "    classifier.add(Dense(units=3, kernel_initializer='uniform', activation='softmax')) # Output layer. Only 1 ouput category, sigmoid activation to get probability of sureness\n",
    "    # Note: Softmax applies to a dependent variable that has more than 2 categories\n",
    "    # i.e. fMRI categorizations\n",
    "    \n",
    "    classifier.compile(loss='categorical_crossentropy',\n",
    "              optimizer=parameters['optimizer'],\n",
    "              metrics=['accuracy'])\n",
    "    # Notes\n",
    "    # adam is a kind of stochastic gradient descent\n",
    "    # For multivariabel, use categorical cross entropy\n",
    "    # Accuracy is predefined\n",
    "    return classifier\n",
    "# Creates a standard Keras type classifier composed of the defined network for\n",
    "# k-means testing\n",
    "data = data_handler('./atlas.pkl', StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 207847 samples, validate on 207847 samples\n",
      "Epoch 1/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.8546 - acc: 0.5968 - val_loss: 0.8040 - val_acc: 0.6012\n",
      "Epoch 2/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.8064 - acc: 0.6003 - val_loss: 0.8027 - val_acc: 0.6016\n",
      "Epoch 3/50\n",
      "207847/207847 [==============================] - 52s 250us/step - loss: 0.8054 - acc: 0.6005 - val_loss: 0.8018 - val_acc: 0.6017\n",
      "Epoch 4/50\n",
      "207847/207847 [==============================] - 52s 250us/step - loss: 0.8039 - acc: 0.6005 - val_loss: 0.8002 - val_acc: 0.6017\n",
      "Epoch 5/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.8013 - acc: 0.6018 - val_loss: 0.7959 - val_acc: 0.6035\n",
      "Epoch 6/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.7959 - acc: 0.6098 - val_loss: 0.7867 - val_acc: 0.6196\n",
      "Epoch 7/50\n",
      "207847/207847 [==============================] - 54s 260us/step - loss: 0.7888 - acc: 0.6225 - val_loss: 0.7779 - val_acc: 0.6325\n",
      "Epoch 8/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.7806 - acc: 0.6344 - val_loss: 0.7670 - val_acc: 0.6446\n",
      "Epoch 9/50\n",
      "207847/207847 [==============================] - 52s 252us/step - loss: 0.7714 - acc: 0.6409 - val_loss: 0.7552 - val_acc: 0.6538\n",
      "Epoch 10/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.7609 - acc: 0.6496 - val_loss: 0.7427 - val_acc: 0.6616\n",
      "Epoch 11/50\n",
      "207847/207847 [==============================] - 48s 233us/step - loss: 0.7518 - acc: 0.6557 - val_loss: 0.7319 - val_acc: 0.6674\n",
      "Epoch 12/50\n",
      "207847/207847 [==============================] - 48s 230us/step - loss: 0.7424 - acc: 0.6609 - val_loss: 0.7237 - val_acc: 0.6711\n",
      "Epoch 13/50\n",
      "207847/207847 [==============================] - 48s 232us/step - loss: 0.7365 - acc: 0.6642 - val_loss: 0.7169 - val_acc: 0.6746\n",
      "Epoch 14/50\n",
      "207847/207847 [==============================] - 48s 232us/step - loss: 0.7293 - acc: 0.6683 - val_loss: 0.7105 - val_acc: 0.6776\n",
      "Epoch 15/50\n",
      "207847/207847 [==============================] - 50s 243us/step - loss: 0.7252 - acc: 0.6709 - val_loss: 0.7058 - val_acc: 0.6798\n",
      "Epoch 16/50\n",
      "207847/207847 [==============================] - 55s 264us/step - loss: 0.7193 - acc: 0.6734 - val_loss: 0.7004 - val_acc: 0.6824\n",
      "Epoch 17/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.7153 - acc: 0.6754 - val_loss: 0.6960 - val_acc: 0.6841\n",
      "Epoch 18/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.7110 - acc: 0.6763 - val_loss: 0.6918 - val_acc: 0.6856\n",
      "Epoch 19/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.7077 - acc: 0.6799 - val_loss: 0.6884 - val_acc: 0.6870\n",
      "Epoch 20/50\n",
      "207847/207847 [==============================] - 51s 246us/step - loss: 0.7039 - acc: 0.6805 - val_loss: 0.6855 - val_acc: 0.6885\n",
      "Epoch 21/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.7016 - acc: 0.6817 - val_loss: 0.6827 - val_acc: 0.6892\n",
      "Epoch 22/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6985 - acc: 0.6836 - val_loss: 0.6800 - val_acc: 0.6906\n",
      "Epoch 23/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6947 - acc: 0.6855 - val_loss: 0.6777 - val_acc: 0.6919\n",
      "Epoch 24/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6919 - acc: 0.6866 - val_loss: 0.6748 - val_acc: 0.6932\n",
      "Epoch 25/50\n",
      "207847/207847 [==============================] - 57s 273us/step - loss: 0.6899 - acc: 0.6873 - val_loss: 0.6734 - val_acc: 0.6939\n",
      "Epoch 26/50\n",
      "207847/207847 [==============================] - 55s 264us/step - loss: 0.6872 - acc: 0.6891 - val_loss: 0.6715 - val_acc: 0.6947\n",
      "Epoch 27/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6858 - acc: 0.6903 - val_loss: 0.6699 - val_acc: 0.6952\n",
      "Epoch 28/50\n",
      "207847/207847 [==============================] - 49s 238us/step - loss: 0.6828 - acc: 0.6921 - val_loss: 0.6678 - val_acc: 0.6962\n",
      "Epoch 29/50\n",
      "207847/207847 [==============================] - 49s 238us/step - loss: 0.6805 - acc: 0.6931 - val_loss: 0.6658 - val_acc: 0.6971\n",
      "Epoch 30/50\n",
      "207847/207847 [==============================] - 49s 238us/step - loss: 0.6790 - acc: 0.6924 - val_loss: 0.6646 - val_acc: 0.6977\n",
      "Epoch 31/50\n",
      "207847/207847 [==============================] - 49s 238us/step - loss: 0.6767 - acc: 0.6941 - val_loss: 0.6635 - val_acc: 0.6983\n",
      "Epoch 32/50\n",
      "207847/207847 [==============================] - 49s 237us/step - loss: 0.6742 - acc: 0.6955 - val_loss: 0.6618 - val_acc: 0.6988\n",
      "Epoch 33/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6724 - acc: 0.6966 - val_loss: 0.6606 - val_acc: 0.7005\n",
      "Epoch 34/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6709 - acc: 0.6970 - val_loss: 0.6596 - val_acc: 0.6999\n",
      "Epoch 35/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6679 - acc: 0.6983 - val_loss: 0.6591 - val_acc: 0.6995\n",
      "Epoch 36/50\n",
      "207847/207847 [==============================] - 49s 237us/step - loss: 0.6666 - acc: 0.6992 - val_loss: 0.6568 - val_acc: 0.7009\n",
      "Epoch 37/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6649 - acc: 0.7006 - val_loss: 0.6550 - val_acc: 0.7026\n",
      "Epoch 38/50\n",
      "207847/207847 [==============================] - 50s 238us/step - loss: 0.6631 - acc: 0.7006 - val_loss: 0.6542 - val_acc: 0.7027\n",
      "Epoch 39/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6624 - acc: 0.7012 - val_loss: 0.6538 - val_acc: 0.7027\n",
      "Epoch 40/50\n",
      "207847/207847 [==============================] - 50s 240us/step - loss: 0.6597 - acc: 0.7021 - val_loss: 0.6518 - val_acc: 0.7033\n",
      "Epoch 41/50\n",
      "207847/207847 [==============================] - 50s 240us/step - loss: 0.6579 - acc: 0.7037 - val_loss: 0.6513 - val_acc: 0.7040\n",
      "Epoch 42/50\n",
      "207847/207847 [==============================] - 50s 242us/step - loss: 0.6575 - acc: 0.7033 - val_loss: 0.6501 - val_acc: 0.7040\n",
      "Epoch 43/50\n",
      "207847/207847 [==============================] - 49s 237us/step - loss: 0.6559 - acc: 0.7046 - val_loss: 0.6495 - val_acc: 0.7044\n",
      "Epoch 44/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6540 - acc: 0.7050 - val_loss: 0.6492 - val_acc: 0.7042\n",
      "Epoch 45/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6534 - acc: 0.7060 - val_loss: 0.6477 - val_acc: 0.7056\n",
      "Epoch 46/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6521 - acc: 0.7066 - val_loss: 0.6468 - val_acc: 0.7062\n",
      "Epoch 47/50\n",
      "207847/207847 [==============================] - 50s 242us/step - loss: 0.6488 - acc: 0.7075 - val_loss: 0.6460 - val_acc: 0.7064\n",
      "Epoch 48/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6484 - acc: 0.7071 - val_loss: 0.6457 - val_acc: 0.7061\n",
      "Epoch 49/50\n",
      "207847/207847 [==============================] - 50s 238us/step - loss: 0.6468 - acc: 0.7085 - val_loss: 0.6451 - val_acc: 0.7068\n",
      "Epoch 50/50\n",
      "207847/207847 [==============================] - 49s 238us/step - loss: 0.6468 - acc: 0.7096 - val_loss: 0.6457 - val_acc: 0.7057\n",
      "       0  1  2\n",
      "0      1  0  0\n",
      "1      0  1  0\n",
      "2      1  0  0\n",
      "3      0  0  1\n",
      "4      0  1  0\n",
      "5      0  0  1\n",
      "6      0  0  1\n",
      "7      0  1  0\n",
      "8      0  1  0\n",
      "9      1  0  0\n",
      "10     0  1  0\n",
      "11     0  0  1\n",
      "12     0  1  0\n",
      "13     0  0  1\n",
      "14     1  0  0\n",
      "15     1  0  0\n",
      "16     0  1  0\n",
      "17     1  0  0\n",
      "18     0  1  0\n",
      "19     0  1  0\n",
      "20     0  1  0\n",
      "21     0  1  0\n",
      "22     1  0  0\n",
      "23     0  1  0\n",
      "24     1  0  0\n",
      "25     0  1  0\n",
      "26     0  0  1\n",
      "27     0  0  1\n",
      "28     0  1  0\n",
      "29     0  0  1\n",
      "...   .. .. ..\n",
      "46158  0  0  1\n",
      "46159  0  0  1\n",
      "46160  0  1  0\n",
      "46161  0  1  0\n",
      "46162  0  0  1\n",
      "46163  0  0  1\n",
      "46164  0  0  1\n",
      "46165  1  0  0\n",
      "46166  0  0  1\n",
      "46167  0  1  0\n",
      "46168  0  0  1\n",
      "46169  1  0  0\n",
      "46170  0  1  0\n",
      "46171  0  0  1\n",
      "46172  1  0  0\n",
      "46173  0  1  0\n",
      "46174  0  1  0\n",
      "46175  0  0  1\n",
      "46176  0  0  1\n",
      "46177  0  0  1\n",
      "46178  0  0  1\n",
      "46179  0  1  0\n",
      "46180  0  0  1\n",
      "46181  1  0  0\n",
      "46182  1  0  0\n",
      "46183  0  1  0\n",
      "46184  1  0  0\n",
      "46185  0  0  1\n",
      "46186  0  0  1\n",
      "46187  0  0  1\n",
      "\n",
      "[46188 rows x 3 columns]\n",
      "Train on 207847 samples, validate on 207847 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.8358 - acc: 0.5988 - val_loss: 0.8036 - val_acc: 0.6014\n",
      "Epoch 2/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.8060 - acc: 0.6009 - val_loss: 0.8026 - val_acc: 0.6017\n",
      "Epoch 3/50\n",
      "207847/207847 [==============================] - 51s 243us/step - loss: 0.8044 - acc: 0.6010 - val_loss: 0.8012 - val_acc: 0.6016\n",
      "Epoch 4/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.8025 - acc: 0.6017 - val_loss: 0.7977 - val_acc: 0.6035\n",
      "Epoch 5/50\n",
      "207847/207847 [==============================] - 51s 244us/step - loss: 0.7972 - acc: 0.6115 - val_loss: 0.7886 - val_acc: 0.6217\n",
      "Epoch 6/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.7888 - acc: 0.6262 - val_loss: 0.7783 - val_acc: 0.6379\n",
      "Epoch 7/50\n",
      "207847/207847 [==============================] - 51s 244us/step - loss: 0.7811 - acc: 0.6360 - val_loss: 0.7700 - val_acc: 0.6453\n",
      "Epoch 8/50\n",
      "207847/207847 [==============================] - 50s 243us/step - loss: 0.7730 - acc: 0.6445 - val_loss: 0.7607 - val_acc: 0.6535\n",
      "Epoch 9/50\n",
      "207847/207847 [==============================] - 53s 257us/step - loss: 0.7661 - acc: 0.6498 - val_loss: 0.7520 - val_acc: 0.6589\n",
      "Epoch 10/50\n",
      "207847/207847 [==============================] - 53s 253us/step - loss: 0.7591 - acc: 0.6541 - val_loss: 0.7425 - val_acc: 0.6642\n",
      "Epoch 11/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.7518 - acc: 0.6573 - val_loss: 0.7337 - val_acc: 0.6678\n",
      "Epoch 12/50\n",
      "207847/207847 [==============================] - 51s 243us/step - loss: 0.7441 - acc: 0.6615 - val_loss: 0.7253 - val_acc: 0.6712\n",
      "Epoch 13/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.7365 - acc: 0.6646 - val_loss: 0.7169 - val_acc: 0.6757\n",
      "Epoch 14/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.7307 - acc: 0.6674 - val_loss: 0.7108 - val_acc: 0.6781\n",
      "Epoch 15/50\n",
      "207847/207847 [==============================] - 51s 244us/step - loss: 0.7247 - acc: 0.6713 - val_loss: 0.7054 - val_acc: 0.6798\n",
      "Epoch 16/50\n",
      "207847/207847 [==============================] - 51s 244us/step - loss: 0.7208 - acc: 0.6723 - val_loss: 0.7013 - val_acc: 0.6816\n",
      "Epoch 17/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.7169 - acc: 0.6743 - val_loss: 0.6966 - val_acc: 0.6831\n",
      "Epoch 18/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.7118 - acc: 0.6751 - val_loss: 0.6935 - val_acc: 0.6842\n",
      "Epoch 19/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.7093 - acc: 0.6770 - val_loss: 0.6906 - val_acc: 0.6846\n",
      "Epoch 20/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.7055 - acc: 0.6787 - val_loss: 0.6879 - val_acc: 0.6860\n",
      "Epoch 21/50\n",
      "207847/207847 [==============================] - 52s 252us/step - loss: 0.7028 - acc: 0.6809 - val_loss: 0.6844 - val_acc: 0.6877\n",
      "Epoch 22/50\n",
      "207847/207847 [==============================] - 51s 248us/step - loss: 0.6999 - acc: 0.6818 - val_loss: 0.6824 - val_acc: 0.6882\n",
      "Epoch 23/50\n",
      "207847/207847 [==============================] - 52s 250us/step - loss: 0.6972 - acc: 0.6833 - val_loss: 0.6800 - val_acc: 0.6894\n",
      "Epoch 24/50\n",
      "207847/207847 [==============================] - 51s 246us/step - loss: 0.6946 - acc: 0.6846 - val_loss: 0.6775 - val_acc: 0.6908\n",
      "Epoch 25/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6916 - acc: 0.6857 - val_loss: 0.6756 - val_acc: 0.6911\n",
      "Epoch 26/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.6897 - acc: 0.6871 - val_loss: 0.6742 - val_acc: 0.6919\n",
      "Epoch 27/50\n",
      "207847/207847 [==============================] - 52s 252us/step - loss: 0.6869 - acc: 0.6888 - val_loss: 0.6715 - val_acc: 0.6932\n",
      "Epoch 28/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6855 - acc: 0.6882 - val_loss: 0.6703 - val_acc: 0.6936\n",
      "Epoch 29/50\n",
      "207847/207847 [==============================] - 57s 272us/step - loss: 0.6828 - acc: 0.6898 - val_loss: 0.6693 - val_acc: 0.6942\n",
      "Epoch 30/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.6814 - acc: 0.6912 - val_loss: 0.6670 - val_acc: 0.6953\n",
      "Epoch 31/50\n",
      "207847/207847 [==============================] - 54s 262us/step - loss: 0.6795 - acc: 0.6913 - val_loss: 0.6668 - val_acc: 0.6951\n",
      "Epoch 32/50\n",
      "207847/207847 [==============================] - 55s 266us/step - loss: 0.6788 - acc: 0.6922 - val_loss: 0.6647 - val_acc: 0.6963\n",
      "Epoch 33/50\n",
      "207847/207847 [==============================] - 54s 260us/step - loss: 0.6764 - acc: 0.6934 - val_loss: 0.6622 - val_acc: 0.6980\n",
      "Epoch 34/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.6729 - acc: 0.6950 - val_loss: 0.6613 - val_acc: 0.6979\n",
      "Epoch 35/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.6715 - acc: 0.6956 - val_loss: 0.6597 - val_acc: 0.6987\n",
      "Epoch 36/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6708 - acc: 0.6962 - val_loss: 0.6591 - val_acc: 0.6991\n",
      "Epoch 37/50\n",
      "207847/207847 [==============================] - 52s 250us/step - loss: 0.6685 - acc: 0.6968 - val_loss: 0.6579 - val_acc: 0.7000\n",
      "Epoch 38/50\n",
      "207847/207847 [==============================] - 53s 253us/step - loss: 0.6668 - acc: 0.6975 - val_loss: 0.6573 - val_acc: 0.6996\n",
      "Epoch 39/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.6640 - acc: 0.6985 - val_loss: 0.6553 - val_acc: 0.7012\n",
      "Epoch 40/50\n",
      "207847/207847 [==============================] - 50s 243us/step - loss: 0.6632 - acc: 0.6993 - val_loss: 0.6547 - val_acc: 0.7011\n",
      "Epoch 41/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6619 - acc: 0.7009 - val_loss: 0.6544 - val_acc: 0.7016\n",
      "Epoch 42/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6600 - acc: 0.7006 - val_loss: 0.6526 - val_acc: 0.7021\n",
      "Epoch 43/50\n",
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.6591 - acc: 0.7004 - val_loss: 0.6518 - val_acc: 0.7027\n",
      "Epoch 44/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.6569 - acc: 0.7019 - val_loss: 0.6503 - val_acc: 0.7038\n",
      "Epoch 45/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.6561 - acc: 0.7017 - val_loss: 0.6508 - val_acc: 0.7029\n",
      "Epoch 46/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.6546 - acc: 0.7026 - val_loss: 0.6494 - val_acc: 0.7037\n",
      "Epoch 47/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6530 - acc: 0.7043 - val_loss: 0.6490 - val_acc: 0.7037\n",
      "Epoch 48/50\n",
      "207847/207847 [==============================] - 53s 256us/step - loss: 0.6526 - acc: 0.7041 - val_loss: 0.6483 - val_acc: 0.7044\n",
      "Epoch 49/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.6497 - acc: 0.7050 - val_loss: 0.6464 - val_acc: 0.7055\n",
      "Epoch 50/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6481 - acc: 0.7058 - val_loss: 0.6455 - val_acc: 0.7065\n",
      "       0  1  2\n",
      "0      1  0  0\n",
      "1      0  1  0\n",
      "2      0  0  1\n",
      "3      0  0  1\n",
      "4      0  0  1\n",
      "5      0  1  0\n",
      "6      0  0  1\n",
      "7      1  0  0\n",
      "8      1  0  0\n",
      "9      1  0  0\n",
      "10     1  0  0\n",
      "11     0  0  1\n",
      "12     1  0  0\n",
      "13     0  1  0\n",
      "14     1  0  0\n",
      "15     0  0  1\n",
      "16     0  1  0\n",
      "17     0  0  1\n",
      "18     0  1  0\n",
      "19     0  0  1\n",
      "20     0  0  1\n",
      "21     0  1  0\n",
      "22     0  0  1\n",
      "23     1  0  0\n",
      "24     0  1  0\n",
      "25     0  0  1\n",
      "26     0  1  0\n",
      "27     1  0  0\n",
      "28     1  0  0\n",
      "29     0  0  1\n",
      "...   .. .. ..\n",
      "46158  0  0  1\n",
      "46159  0  0  1\n",
      "46160  0  1  0\n",
      "46161  1  0  0\n",
      "46162  0  1  0\n",
      "46163  0  0  1\n",
      "46164  0  0  1\n",
      "46165  0  0  1\n",
      "46166  0  1  0\n",
      "46167  0  0  1\n",
      "46168  0  0  1\n",
      "46169  0  1  0\n",
      "46170  0  0  1\n",
      "46171  1  0  0\n",
      "46172  0  0  1\n",
      "46173  0  1  0\n",
      "46174  0  1  0\n",
      "46175  0  1  0\n",
      "46176  0  0  1\n",
      "46177  1  0  0\n",
      "46178  0  1  0\n",
      "46179  1  0  0\n",
      "46180  0  0  1\n",
      "46181  0  0  1\n",
      "46182  0  0  1\n",
      "46183  0  0  1\n",
      "46184  1  0  0\n",
      "46185  0  1  0\n",
      "46186  0  0  1\n",
      "46187  1  0  0\n",
      "\n",
      "[46188 rows x 3 columns]\n",
      "Train on 207847 samples, validate on 207847 samples\n",
      "Epoch 1/50\n",
      "207847/207847 [==============================] - 54s 258us/step - loss: 0.8359 - acc: 0.5984 - val_loss: 0.8037 - val_acc: 0.6015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "207847/207847 [==============================] - 53s 256us/step - loss: 0.8055 - acc: 0.6005 - val_loss: 0.8027 - val_acc: 0.6016\n",
      "Epoch 3/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.8046 - acc: 0.6007 - val_loss: 0.8018 - val_acc: 0.6017\n",
      "Epoch 4/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.8034 - acc: 0.6006 - val_loss: 0.7996 - val_acc: 0.6016\n",
      "Epoch 5/50\n",
      "207847/207847 [==============================] - 52s 250us/step - loss: 0.7998 - acc: 0.6054 - val_loss: 0.7927 - val_acc: 0.6134\n",
      "Epoch 6/50\n",
      "207847/207847 [==============================] - 52s 250us/step - loss: 0.7927 - acc: 0.6189 - val_loss: 0.7822 - val_acc: 0.6300\n",
      "Epoch 7/50\n",
      "207847/207847 [==============================] - 51s 246us/step - loss: 0.7837 - acc: 0.6314 - val_loss: 0.7706 - val_acc: 0.6436\n",
      "Epoch 8/50\n",
      "207847/207847 [==============================] - 54s 258us/step - loss: 0.7743 - acc: 0.6406 - val_loss: 0.7599 - val_acc: 0.6521\n",
      "Epoch 9/50\n",
      "207847/207847 [==============================] - 53s 253us/step - loss: 0.7650 - acc: 0.6468 - val_loss: 0.7480 - val_acc: 0.6589\n",
      "Epoch 10/50\n",
      "207847/207847 [==============================] - 53s 253us/step - loss: 0.7547 - acc: 0.6533 - val_loss: 0.7354 - val_acc: 0.6656\n",
      "Epoch 11/50\n",
      "207847/207847 [==============================] - 54s 259us/step - loss: 0.7451 - acc: 0.6585 - val_loss: 0.7255 - val_acc: 0.6710\n",
      "Epoch 12/50\n",
      "207847/207847 [==============================] - 54s 262us/step - loss: 0.7381 - acc: 0.6622 - val_loss: 0.7182 - val_acc: 0.6739\n",
      "Epoch 13/50\n",
      "207847/207847 [==============================] - 53s 255us/step - loss: 0.7306 - acc: 0.6658 - val_loss: 0.7109 - val_acc: 0.6769\n",
      "Epoch 14/50\n",
      "207847/207847 [==============================] - 56s 270us/step - loss: 0.7257 - acc: 0.6688 - val_loss: 0.7056 - val_acc: 0.6787\n",
      "Epoch 15/50\n",
      "207847/207847 [==============================] - 50s 242us/step - loss: 0.7195 - acc: 0.6709 - val_loss: 0.7009 - val_acc: 0.6817\n",
      "Epoch 16/50\n",
      "207847/207847 [==============================] - 51s 244us/step - loss: 0.7152 - acc: 0.6734 - val_loss: 0.6963 - val_acc: 0.6830\n",
      "Epoch 17/50\n",
      "207847/207847 [==============================] - 51s 244us/step - loss: 0.7116 - acc: 0.6753 - val_loss: 0.6926 - val_acc: 0.6842\n",
      "Epoch 18/50\n",
      "207847/207847 [==============================] - 53s 256us/step - loss: 0.7078 - acc: 0.6779 - val_loss: 0.6890 - val_acc: 0.6861\n",
      "Epoch 19/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.7040 - acc: 0.6799 - val_loss: 0.6861 - val_acc: 0.6877\n",
      "Epoch 20/50\n",
      "207847/207847 [==============================] - 53s 256us/step - loss: 0.7015 - acc: 0.6807 - val_loss: 0.6831 - val_acc: 0.6890\n",
      "Epoch 21/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6988 - acc: 0.6826 - val_loss: 0.6813 - val_acc: 0.6895\n",
      "Epoch 22/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.6952 - acc: 0.6837 - val_loss: 0.6779 - val_acc: 0.6915\n",
      "Epoch 23/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.6939 - acc: 0.6842 - val_loss: 0.6759 - val_acc: 0.6919\n",
      "Epoch 24/50\n",
      "207847/207847 [==============================] - 52s 250us/step - loss: 0.6894 - acc: 0.6862 - val_loss: 0.6749 - val_acc: 0.6922\n",
      "Epoch 25/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6891 - acc: 0.6878 - val_loss: 0.6718 - val_acc: 0.6938\n",
      "Epoch 26/50\n",
      "207847/207847 [==============================] - 52s 248us/step - loss: 0.6856 - acc: 0.6888 - val_loss: 0.6698 - val_acc: 0.6948\n",
      "Epoch 27/50\n",
      "207847/207847 [==============================] - 53s 254us/step - loss: 0.6826 - acc: 0.6892 - val_loss: 0.6681 - val_acc: 0.6958\n",
      "Epoch 28/50\n",
      "207847/207847 [==============================] - 51s 244us/step - loss: 0.6803 - acc: 0.6906 - val_loss: 0.6664 - val_acc: 0.6971\n",
      "Epoch 29/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6789 - acc: 0.6921 - val_loss: 0.6652 - val_acc: 0.6970\n",
      "Epoch 30/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6777 - acc: 0.6927 - val_loss: 0.6642 - val_acc: 0.6973\n",
      "Epoch 31/50\n",
      "207847/207847 [==============================] - 53s 254us/step - loss: 0.6752 - acc: 0.6940 - val_loss: 0.6621 - val_acc: 0.6985\n",
      "Epoch 32/50\n",
      "207847/207847 [==============================] - 51s 245us/step - loss: 0.6730 - acc: 0.6951 - val_loss: 0.6603 - val_acc: 0.6992\n",
      "Epoch 33/50\n",
      "207847/207847 [==============================] - 51s 246us/step - loss: 0.6705 - acc: 0.6961 - val_loss: 0.6593 - val_acc: 0.6996\n",
      "Epoch 34/50\n",
      "207847/207847 [==============================] - 52s 249us/step - loss: 0.6690 - acc: 0.6966 - val_loss: 0.6587 - val_acc: 0.7001\n",
      "Epoch 35/50\n",
      "207847/207847 [==============================] - 56s 270us/step - loss: 0.6670 - acc: 0.6968 - val_loss: 0.6572 - val_acc: 0.7002\n",
      "Epoch 36/50\n",
      "207847/207847 [==============================] - 52s 252us/step - loss: 0.6652 - acc: 0.6981 - val_loss: 0.6563 - val_acc: 0.7010\n",
      "Epoch 37/50\n",
      "207847/207847 [==============================] - 52s 251us/step - loss: 0.6655 - acc: 0.6976 - val_loss: 0.6546 - val_acc: 0.7019\n",
      "Epoch 38/50\n",
      "207847/207847 [==============================] - 51s 246us/step - loss: 0.6626 - acc: 0.7007 - val_loss: 0.6534 - val_acc: 0.7026\n",
      "Epoch 39/50\n",
      "207847/207847 [==============================] - 50s 242us/step - loss: 0.6605 - acc: 0.7011 - val_loss: 0.6524 - val_acc: 0.7024\n",
      "Epoch 40/50\n",
      "207847/207847 [==============================] - 50s 240us/step - loss: 0.6588 - acc: 0.7007 - val_loss: 0.6517 - val_acc: 0.7031\n",
      "Epoch 41/50\n",
      "207847/207847 [==============================] - 50s 239us/step - loss: 0.6584 - acc: 0.7009 - val_loss: 0.6512 - val_acc: 0.7033\n",
      "Epoch 42/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6574 - acc: 0.7020 - val_loss: 0.6496 - val_acc: 0.7047\n",
      "Epoch 43/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6544 - acc: 0.7031 - val_loss: 0.6485 - val_acc: 0.7047\n",
      "Epoch 44/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6542 - acc: 0.7040 - val_loss: 0.6480 - val_acc: 0.7047\n",
      "Epoch 45/50\n",
      "207847/207847 [==============================] - 50s 240us/step - loss: 0.6520 - acc: 0.7039 - val_loss: 0.6467 - val_acc: 0.7059\n",
      "Epoch 46/50\n",
      "207847/207847 [==============================] - 50s 238us/step - loss: 0.6504 - acc: 0.7058 - val_loss: 0.6453 - val_acc: 0.7058\n",
      "Epoch 47/50\n",
      "207847/207847 [==============================] - 50s 242us/step - loss: 0.6495 - acc: 0.7061 - val_loss: 0.6451 - val_acc: 0.7063\n",
      "Epoch 48/50\n",
      "207847/207847 [==============================] - 50s 240us/step - loss: 0.6478 - acc: 0.7073 - val_loss: 0.6449 - val_acc: 0.7064\n",
      "Epoch 49/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6469 - acc: 0.7071 - val_loss: 0.6433 - val_acc: 0.7075\n",
      "Epoch 50/50\n",
      "207847/207847 [==============================] - 50s 241us/step - loss: 0.6447 - acc: 0.7080 - val_loss: 0.6440 - val_acc: 0.7067\n",
      "       0  1  2\n",
      "0      0  1  0\n",
      "1      0  0  1\n",
      "2      1  0  0\n",
      "3      0  1  0\n",
      "4      1  0  0\n",
      "5      0  0  1\n",
      "6      0  1  0\n",
      "7      1  0  0\n",
      "8      0  0  1\n",
      "9      0  0  1\n",
      "10     0  1  0\n",
      "11     0  1  0\n",
      "12     0  0  1\n",
      "13     0  0  1\n",
      "14     1  0  0\n",
      "15     0  1  0\n",
      "16     0  0  1\n",
      "17     0  0  1\n",
      "18     1  0  0\n",
      "19     0  0  1\n",
      "20     0  1  0\n",
      "21     1  0  0\n",
      "22     0  1  0\n",
      "23     1  0  0\n",
      "24     1  0  0\n",
      "25     1  0  0\n",
      "26     1  0  0\n",
      "27     0  1  0\n",
      "28     0  0  1\n",
      "29     0  0  1\n",
      "...   .. .. ..\n",
      "46158  0  1  0\n",
      "46159  1  0  0\n",
      "46160  0  0  1\n",
      "46161  0  1  0\n",
      "46162  1  0  0\n",
      "46163  1  0  0\n",
      "46164  0  0  1\n",
      "46165  1  0  0\n",
      "46166  1  0  0\n",
      "46167  0  1  0\n",
      "46168  0  0  1\n",
      "46169  1  0  0\n",
      "46170  0  0  1\n",
      "46171  0  0  1\n",
      "46172  0  1  0\n",
      "46173  0  0  1\n",
      "46174  0  0  1\n",
      "46175  0  1  0\n",
      "46176  0  1  0\n",
      "46177  0  0  1\n",
      "46178  0  0  1\n",
      "46179  1  0  0\n",
      "46180  0  0  1\n",
      "46181  1  0  0\n",
      "46182  0  0  1\n",
      "46183  0  0  1\n",
      "46184  1  0  0\n",
      "46185  0  1  0\n",
      "46186  1  0  0\n",
      "46187  0  0  1\n",
      "\n",
      "[46188 rows x 3 columns]\n",
      "Train on 207847 samples, validate on 207847 samples\n",
      "Epoch 1/50\n",
      "207847/207847 [==============================] - 51s 246us/step - loss: 0.8354 - acc: 0.5983 - val_loss: 0.8036 - val_acc: 0.6015\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207847/207847 [==============================] - 51s 247us/step - loss: 0.8057 - acc: 0.6003 - val_loss: 0.8028 - val_acc: 0.6016\n",
      "Epoch 3/50\n",
      "117040/207847 [===============>..............] - ETA: 19s - loss: 0.8049 - acc: 0.6005"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-afc5d55821b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cross_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m#test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         if (self._delta_t_batch > 0. and\n\u001b[1;32m    117\u001b[0m            (delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1)):\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mmedian\u001b[0;34m(a, axis, out, overwrite_input, keepdims)\u001b[0m\n\u001b[1;32m   3334\u001b[0m     \"\"\"\n\u001b[1;32m   3335\u001b[0m     r, k = _ureduce(a, func=_median, axis=axis, out=out,\n\u001b[0;32m-> 3336\u001b[0;31m                     overwrite_input=overwrite_input)\n\u001b[0m\u001b[1;32m   3337\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_ureduce\u001b[0;34m(a, func, **kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m         \u001b[0mkeepdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3250\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3251\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36m_median\u001b[0;34m(a, axis, out, overwrite_input)\u001b[0m\n\u001b[1;32m   3356\u001b[0m         \u001b[0mkth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3357\u001b[0m     \u001b[0;31m# Check if the array contains any nan's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3358\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minexact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3359\u001b[0m         \u001b[0mkth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/numpy/core/numerictypes.py\u001b[0m in \u001b[0;36missubdtype\u001b[0;34m(arg1, arg2)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \"\"\"\n\u001b[0;32m--> 713\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(0, 10):\n",
    "    i = i + 1\n",
    "    #train\n",
    "    tensorboard = TensorBoard(log_dir=\"../logs/{}\".format(time.time()))\n",
    "    x_train, y_train, x_test, y_test = data.get_cross_val(i)\n",
    "    classifier = build_classifier(parameters)\n",
    "    classifier.fit(x_train, y_train, batch_size=parameters['batch_size'], epochs=parameters['epochs'],callbacks=[tensorboard], validation_split=0.5)\n",
    "    \n",
    "    #test\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    y_pred = pd.DataFrame(y_pred)\n",
    "    y_test = pd.DataFrame(y_test)\n",
    "    print(y_test)\n",
    "    y_pred['test_0'] = y_test.iloc[:, [0]]\n",
    "    y_pred['test_1'] = y_test.iloc[:, [1]]\n",
    "    y_pred['test_2'] = y_test.iloc[:, [2]]\n",
    "    \n",
    "    y_pred.to_pickle('./Cross_Validation_Results/cv_results_' + str(i - 1) + '.pkl')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classifier.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
