{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0         1         2         3          4         5  \\\n",
      "0       -0.432690 -1.297000 -1.062600 -0.659030   0.749150  0.668930   \n",
      "1        0.048841  0.017907  0.063133  0.066374  -0.021142  0.054850   \n",
      "2        0.018148 -0.002884 -0.112950  0.066071   0.000690 -0.025123   \n",
      "3        0.077216  0.155570  0.148840 -0.078220  -0.046353  0.056544   \n",
      "4       -0.007275  0.001259 -0.003566 -0.039139  -0.029627 -0.021410   \n",
      "5       -0.032138 -0.080774 -0.080089  0.067738   0.010799 -0.014069   \n",
      "6       -0.094736 -0.100570 -0.071752 -0.052315   0.023642  0.151830   \n",
      "7       -0.142300  0.056452 -0.138510  0.037676  -0.169830  0.056670   \n",
      "8       -0.059631 -0.074320 -0.124400 -0.075413   0.024633  0.027707   \n",
      "9       -0.050301 -0.047367  0.090624 -0.002637  -0.038612 -0.012719   \n",
      "10      -0.129250 -0.162840 -0.019609 -0.038128  -0.009866  0.014628   \n",
      "11       0.012858  0.018466 -0.103660  0.052701   0.001016 -0.044332   \n",
      "12      -0.149640 -0.129800  0.146140 -0.118890  -0.055036 -0.211820   \n",
      "13       0.005734  0.083105  0.118710  0.075920   0.034409  0.023631   \n",
      "14      -0.099226 -0.039744 -0.062816  0.097594  -0.009369  0.112680   \n",
      "15      -0.042200  0.034404  0.133350 -0.041954   0.059561 -0.049417   \n",
      "16       0.028617  0.073590  0.040902  0.086458   0.005406 -0.010775   \n",
      "17      -0.147550 -0.201470 -0.010883 -0.090728  -0.054433 -0.124980   \n",
      "18      -0.022015  0.023577 -0.014323 -0.011360  -0.027378  0.104810   \n",
      "19       0.170760  0.200890  0.151430  0.059658   0.129400  0.068568   \n",
      "20      -0.025188  0.112980 -0.066108 -0.073001   0.131900  0.007917   \n",
      "21       0.026747 -0.059832  0.053580  0.007473  -0.000511  0.032045   \n",
      "22      -5.347300 -8.078600 -4.825700  3.839800   0.717030 -4.588100   \n",
      "23       0.008903  0.074146  0.076728  0.043359   0.095271  0.109110   \n",
      "24      -0.031273 -0.032914 -0.081565 -0.082729  -0.021774 -0.081343   \n",
      "25       0.019728  0.041815  0.030297  0.002916   0.031232  0.069393   \n",
      "26      -0.077169 -0.021728  0.045349 -0.065545  -0.008720 -0.043895   \n",
      "27       1.775100  6.231600  0.189870 -1.373200  -1.684200 -7.037600   \n",
      "28       0.019259 -0.019480 -0.017564  0.014521  -0.058784  0.008508   \n",
      "29      -0.004586 -0.024813  0.029562 -0.002154   0.021232 -0.029398   \n",
      "...           ...       ...       ...       ...        ...       ...   \n",
      "461852  -0.019812 -0.019562 -0.048893 -0.172290  -0.010043 -0.085653   \n",
      "461853  -0.035810 -0.000027 -0.016394 -0.026309   0.046532 -0.044872   \n",
      "461854  -0.015630 -0.052197 -0.120990 -0.092347  -0.034169  0.045354   \n",
      "461855   0.072274  0.090086  0.156760  0.077030  -0.039520  0.050501   \n",
      "461856   0.003777 -0.083604 -0.014814  0.110390  -0.066633  0.204020   \n",
      "461857   0.033008  0.020948  0.025280 -0.085095  -0.053235 -0.103880   \n",
      "461858   0.095110  0.014953  0.065702  0.043306   0.016827 -0.071350   \n",
      "461859  -0.091118 -0.131470  0.011245 -0.056976   0.000972  0.060072   \n",
      "461860  -0.023248  0.015542 -0.032030  0.084439   0.020250 -0.051312   \n",
      "461861  -0.013325 -0.064986  0.065991 -0.063622  -0.077040 -0.150580   \n",
      "461862   0.052730 -0.007614 -0.001104  0.030835   0.077679  0.037915   \n",
      "461863   0.036023  0.011056 -0.099151  0.089556  -0.117930 -0.058992   \n",
      "461864   0.610040  3.063500 -2.877300  4.242200  18.566000  5.853800   \n",
      "461865  -0.188770 -0.146130 -0.039617 -0.052422  -0.129670 -0.019123   \n",
      "461866   1.643900  0.072469 -2.207600 -3.723300  -0.967640 -3.045800   \n",
      "461867   0.034738 -0.084561  0.028058  0.024479  -0.219300 -0.009609   \n",
      "461868  -5.042600 -7.111200 -4.487600 -0.732820   0.213710 -4.435300   \n",
      "461869  -0.074473 -0.074848  0.134900  0.087158   0.123110  0.020308   \n",
      "461870   0.081689  0.041786  0.043391  0.161410   0.023147  0.021484   \n",
      "461871  -0.045449 -0.034756 -0.040099 -0.004761  -0.106810 -0.049910   \n",
      "461872  -0.096008 -0.052183  0.070050 -0.053529   0.078883 -0.018750   \n",
      "461873   0.039717  0.026663  0.054993  0.026177  -0.031361  0.011835   \n",
      "461874   0.044375 -0.011037 -0.002608  0.001455  -0.058898  0.005777   \n",
      "461875   0.140130  0.123630  0.049824  0.077079   0.108740  0.125780   \n",
      "461876   0.107810  0.071323  0.000496  0.095897   0.060143 -0.029286   \n",
      "461877  -0.002944 -0.013169  0.019829 -0.023858   0.041330  0.055077   \n",
      "461878   0.020641  0.116920  0.028455  0.109370   0.082271  0.034069   \n",
      "461879  -0.048937  0.004021 -0.012422 -0.085009  -0.061682  0.128000   \n",
      "461880 -15.418000  2.873900 -2.518700  0.425410   0.887510  1.038700   \n",
      "461881  -0.077554 -0.150360 -0.101550 -0.070229   0.009633  0.001712   \n",
      "\n",
      "                6         7         8         9  ...          257       258  \\\n",
      "0        0.396980 -1.468800 -1.241000 -0.026423  ...    -2.046100 -0.371760   \n",
      "1        0.092258  0.028843  0.083311  0.003993  ...     0.027471  0.062306   \n",
      "2       -0.138470 -0.051747 -0.015151 -0.003125  ...    -0.099523  0.010752   \n",
      "3        0.036682  0.103310  0.165470  0.135520  ...     0.050758  0.006655   \n",
      "4        0.020742 -0.057296 -0.002192  0.094946  ...     0.016821 -0.004939   \n",
      "5       -0.044743  0.014164 -0.117290 -0.124540  ...    -0.052824 -0.046271   \n",
      "6       -0.000946 -0.077902  0.008196 -0.050137  ...    -0.019068 -0.067191   \n",
      "7       -0.045405  0.029568 -0.011090 -0.006872  ...     0.052883 -0.049169   \n",
      "8        0.046917 -0.165870 -0.016461 -0.061244  ...    -0.011277 -0.150950   \n",
      "9       -0.061998 -0.111980  0.009243  0.038462  ...     0.071919  0.033817   \n",
      "10       0.003834 -0.130450 -0.065420 -0.025766  ...     0.042972 -0.060846   \n",
      "11      -0.020769  0.087370 -0.017132 -0.037200  ...     0.076694  0.068758   \n",
      "12       0.036040 -0.040388 -0.122470 -0.112770  ...    -0.256050 -0.043359   \n",
      "13      -0.143140  0.042727  0.031602  0.044010  ...     0.108420  0.008214   \n",
      "14      -0.072364  0.034275 -0.020821  0.001715  ...    -0.074649 -0.140290   \n",
      "15       0.055915  0.018184  0.035096  0.127470  ...     0.021910  0.060335   \n",
      "16       0.112050  0.139310  0.067289  0.058832  ...     0.174220 -0.019080   \n",
      "17      -0.082559 -0.084157 -0.011747  0.018025  ...     0.006352 -0.011197   \n",
      "18       0.010914 -0.032335 -0.022762  0.076279  ...    -0.027021 -0.035996   \n",
      "19      -0.006025  0.210920  0.104000  0.123810  ...     0.064802  0.038188   \n",
      "20      -0.005489 -0.016436  0.034032  0.055401  ...     0.096226  0.014473   \n",
      "21       0.026226  0.070547  0.089485 -0.016239  ...    -0.041131 -0.076579   \n",
      "22      -2.864500 -3.731700  2.880200  2.431600  ...     5.549900 -7.746500   \n",
      "23      -0.001667  0.025219  0.025740 -0.007962  ...     0.035463 -0.065476   \n",
      "24      -0.043514 -0.040343 -0.040982  0.006099  ...    -0.003488  0.010148   \n",
      "25       0.058721 -0.007966  0.060728 -0.021527  ...     0.011422 -0.001454   \n",
      "26       0.010936 -0.074611 -0.026865 -0.050710  ...    -0.085327 -0.076086   \n",
      "27      -5.846900 -3.789300 -0.419620 -6.995600  ...    -3.635900 -7.932300   \n",
      "28      -0.000478 -0.026251 -0.057585 -0.050683  ...     0.019750  0.002379   \n",
      "29      -0.035843  0.086701 -0.018993 -0.052452  ...     0.028673  0.072403   \n",
      "...           ...       ...       ...       ...  ...          ...       ...   \n",
      "461852  -0.010198  0.031013 -0.029703 -0.080340  ...    -0.034287 -0.036241   \n",
      "461853  -0.064680 -0.014661 -0.067392 -0.125030  ...    -0.028345  0.042782   \n",
      "461854   0.067765 -0.043447 -0.118530 -0.100850  ...     0.035927 -0.147700   \n",
      "461855   0.097472  0.081092  0.019098  0.098274  ...     0.056960  0.012901   \n",
      "461856  -0.036530  0.080741  0.051094  0.074053  ...     0.060949  0.075676   \n",
      "461857   0.022854 -0.075878 -0.024574  0.032633  ...     0.002184 -0.058074   \n",
      "461858   0.067220 -0.062928  0.056822 -0.040677  ...    -0.113980 -0.143330   \n",
      "461859  -0.067875 -0.084438 -0.037200 -0.074812  ...     0.012575 -0.084217   \n",
      "461860  -0.035587  0.098975  0.076413  0.035323  ...     0.064731  0.094106   \n",
      "461861  -0.101380 -0.120180 -0.094945 -0.040711  ...     0.049098 -0.125400   \n",
      "461862  -0.006425  0.034320  0.033986 -0.057362  ...    -0.000441  0.043333   \n",
      "461863   0.008317 -0.151110  0.029836  0.051665  ...    -0.115430 -0.100570   \n",
      "461864   3.159400  1.649500  4.531300  6.743700  ...    27.141000  3.663200   \n",
      "461865  -0.038370 -0.027906  0.045469 -0.102480  ...     0.042810  0.074424   \n",
      "461866  -4.714800 -0.727210  0.858230  0.088580  ...     1.819000  5.655000   \n",
      "461867  -0.036575 -0.238650  0.039350 -0.077466  ...     0.019651 -0.030591   \n",
      "461868 -10.021000  0.388620 -9.827100 -1.342400  ...     1.905100  0.440250   \n",
      "461869   0.081998 -0.060793 -0.051609  0.065027  ...     0.043829  0.153360   \n",
      "461870   0.011562  0.096262  0.034639  0.054665  ...     0.084099  0.014804   \n",
      "461871  -0.097763  0.001065 -0.108870 -0.045795  ...    -0.089707  0.003995   \n",
      "461872   0.069127  0.112890  0.076869  0.000914  ...     0.082544  0.184980   \n",
      "461873   0.017936  0.027502  0.019959  0.003995  ...    -0.006390  0.020503   \n",
      "461874   0.008849 -0.027094  0.014381 -0.092854  ...    -0.007727 -0.167080   \n",
      "461875  -0.060355 -0.011982 -0.009781  0.096743  ...     0.053145  0.076121   \n",
      "461876   0.100710  0.034948 -0.020522  0.096133  ...    -0.013892 -0.016875   \n",
      "461877   0.036282  0.009921  0.003215 -0.000763  ...     0.016353 -0.015465   \n",
      "461878  -0.013455  0.033153  0.068142  0.021560  ...     0.106780  0.083759   \n",
      "461879  -0.003550 -0.112530  0.100020 -0.059693  ...    -0.062259  0.077690   \n",
      "461880  -2.698200  0.323000 -6.346200  2.745900  ...     0.414840  0.020865   \n",
      "461881  -0.058171 -0.307130 -0.155640  0.082480  ...    -0.130170 -0.049173   \n",
      "\n",
      "             259       260       261        262       263  Rest  Emote  Solve  \n",
      "0       1.622400 -0.838970  2.491200  -3.650700 -3.338400     1      0      0  \n",
      "1       0.024914  0.002473  0.070595   0.023590  0.029568     0      1      0  \n",
      "2      -0.123000 -0.038730  0.049507  -0.015496 -0.094078     0      1      0  \n",
      "3       0.087858 -0.004605  0.018514  -0.021197 -0.104670     0      0      1  \n",
      "4       0.024965 -0.027829 -0.005525   0.051051 -0.012902     0      0      1  \n",
      "5      -0.036547 -0.124500  0.080867  -0.018044 -0.096131     0      1      0  \n",
      "6      -0.051316 -0.047886  0.020442  -0.081419 -0.020040     0      0      1  \n",
      "7       0.095871  0.011398  0.114910   0.038110  0.065483     0      1      0  \n",
      "8      -0.044679  0.049537  0.111700   0.030890 -0.017603     0      0      1  \n",
      "9       0.040807  0.031106 -0.063868   0.043544 -0.025121     0      0      1  \n",
      "10     -0.122750 -0.036952 -0.077610  -0.019387  0.020641     0      0      1  \n",
      "11      0.057400  0.007100  0.060163  -0.080213 -0.103200     0      0      1  \n",
      "12     -0.139660  0.033637  0.006149  -0.067144 -0.068050     0      0      1  \n",
      "13      0.067867 -0.011631  0.122690   0.069075 -0.003478     0      0      1  \n",
      "14     -0.095591 -0.092845 -0.044095  -0.203410 -0.091424     0      1      0  \n",
      "15      0.042587  0.015622  0.077789   0.037112  0.011675     0      0      1  \n",
      "16      0.069209 -0.008986  0.046728   0.004800  0.066181     0      0      1  \n",
      "17      0.006882 -0.047315 -0.045722  -0.046050 -0.053263     0      1      0  \n",
      "18     -0.054873  0.058154 -0.027123  -0.005015  0.061736     0      1      0  \n",
      "19      0.072182  0.226460  0.041982   0.012271  0.067478     1      0      0  \n",
      "20      0.056062  0.044936  0.061985   0.009485  0.088782     1      0      0  \n",
      "21     -0.070467 -0.101400  0.008624  -0.106960 -0.074881     0      1      0  \n",
      "22     -7.517100 -8.575800  4.202700 -10.517000 -2.033500     1      0      0  \n",
      "23      0.089332  0.056447 -0.027184   0.115920  0.168990     0      0      1  \n",
      "24     -0.013943 -0.010146 -0.019195   0.001529 -0.015513     0      1      0  \n",
      "25      0.032500 -0.020495  0.042974  -0.019025 -0.015917     0      0      1  \n",
      "26     -0.077230 -0.102700 -0.091096  -0.103310 -0.120950     0      0      1  \n",
      "27      1.523800 -3.052700 -3.579600  -1.385500 -0.704220     1      0      0  \n",
      "28     -0.068388 -0.005132  0.034083   0.041468 -0.008148     0      0      1  \n",
      "29     -0.047058 -0.012778 -0.018091  -0.047948  0.021787     0      0      1  \n",
      "...          ...       ...       ...        ...       ...   ...    ...    ...  \n",
      "461852 -0.048357 -0.017453 -0.013199  -0.006781 -0.048354     0      0      1  \n",
      "461853 -0.128270 -0.065534 -0.025394  -0.104750 -0.070044     0      0      1  \n",
      "461854  0.001320 -0.143510  0.023601  -0.067700 -0.097903     0      1      0  \n",
      "461855 -0.029677  0.022550 -0.047255  -0.085306  0.028498     0      0      1  \n",
      "461856 -0.034671 -0.042742 -0.045297   0.064523 -0.152860     0      1      0  \n",
      "461857 -0.118480 -0.099883 -0.014252  -0.048978 -0.009749     0      0      1  \n",
      "461858 -0.073397 -0.084247 -0.000780  -0.067004  0.019065     0      1      0  \n",
      "461859 -0.066703 -0.118300 -0.060749  -0.013244 -0.098118     0      0      1  \n",
      "461860  0.054541  0.021566  0.049172   0.018857  0.058707     0      1      0  \n",
      "461861  0.043317 -0.094900 -0.061704   0.093836 -0.092085     1      0      0  \n",
      "461862  0.126220  0.020250  0.119410   0.080939  0.045429     0      0      1  \n",
      "461863 -0.105570 -0.109140  0.020395  -0.010046 -0.064363     0      1      0  \n",
      "461864  2.407500  7.250800  6.591000  21.219000  8.662500     1      0      0  \n",
      "461865  0.095579  0.137560 -0.046289  -0.162640  0.063670     0      0      1  \n",
      "461866 -1.685300  0.731570 -2.518900  -4.047600 -1.913300     1      0      0  \n",
      "461867 -0.033692 -0.177200 -0.101050  -0.031287 -0.089403     0      1      0  \n",
      "461868 -1.463000 -2.530700 -2.319700  -0.830520 -1.033000     1      0      0  \n",
      "461869  0.014352 -0.009305  0.045962  -0.039560  0.042912     0      1      0  \n",
      "461870 -0.042553  0.082746 -0.097343   0.103950  0.088695     0      1      0  \n",
      "461871  0.028568 -0.013713 -0.048917  -0.071710 -0.020434     1      0      0  \n",
      "461872  0.089482  0.137140  0.018724   0.074575  0.129150     0      0      1  \n",
      "461873 -0.012422  0.009275  0.019106   0.043402  0.020197     0      0      1  \n",
      "461874 -0.027559 -0.205090  0.029075  -0.137650 -0.046146     0      1      0  \n",
      "461875  0.035658  0.088574  0.025570   0.009060  0.098976     0      0      1  \n",
      "461876 -0.015804 -0.010176  0.011526  -0.021270  0.058610     0      0      1  \n",
      "461877 -0.000705 -0.002732  0.003494   0.019302 -0.009399     0      1      0  \n",
      "461878  0.036255  0.005142  0.112430   0.098117  0.018411     0      0      1  \n",
      "461879 -0.008601 -0.070683  0.093169  -0.050948 -0.000816     0      0      1  \n",
      "461880  0.295100  3.091100  1.510300   1.019800  1.615600     1      0      0  \n",
      "461881 -0.072784  0.092810  0.016760  -0.167710 -0.045882     0      0      1  \n",
      "\n",
      "[461882 rows x 267 columns]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "('Rest', 'Emote', 'Solve')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3077\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3078\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('Rest', 'Emote', 'Solve')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-445055cb31d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Creates a standard Keras type classifier composed of the defined network for\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# k-means testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./atlas.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-445055cb31d3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, pkl_location)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m264\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# max 264 x columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Rest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Emote'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Solve'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2687\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2693\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4115\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4117\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3078\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ('Rest', 'Emote', 'Solve')"
     ]
    }
   ],
   "source": [
    "# This is a practice Artificial Neural Network\n",
    "# The problem being solved is based off of a model bank with fake data\n",
    "# The bank has customers that have left for whatever reason\n",
    "# The goal is to find why these customers have left using information such as account balance and gender\n",
    "# The last column of the data states whether or not the customer has left the bank\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np #Math operations library\n",
    "import matplotlib.pyplot as plt #Visualization library\n",
    "import pandas as pd #Matrix handler\n",
    "\n",
    "import keras # Brings in tensorflow with it\n",
    "from keras.models import Sequential # Used for initialization of ANN\n",
    "from keras.layers import Dense, Conv3D, MaxPooling3D, Flatten, Dropout, BatchNormalization# adds layers to ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier # ability to turn network into a function definition\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler #Methods to change categorical strings to numbers and scaling ability\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # Splits data into training and testing\n",
    "from sklearn.metrics import confusion_matrix # Creates truth table for evaluating results\n",
    "\n",
    "parameters = {'batch_size': 100,\n",
    "              'epochs': 100,\n",
    "              'learning_rate': 0.00001,\n",
    "              'nodes': 2400,\n",
    "              'hidden_layers': 2\n",
    "             } # Creates list of parameters to test to find most successful one\n",
    "parameters['optimizer'] = Adam(parameters['learning_rate'])\n",
    "\n",
    "class data_handler:\n",
    "    def __init__(self, pkl_location):\n",
    "        self.data = pd.read_pickle(pkl_location)\n",
    "        self.data = self.data.sample(frac=1).reset_index(drop=True)\n",
    "        print(self.data)\n",
    "        self.x = self.data[list(range(0, 264))]# max 264 x columns\n",
    "        self.y = self.data[['Rest', 'Emote', 'Solve']]\n",
    "        print(self.data)\n",
    "        del self.data\n",
    "        \n",
    "        self.tenth = math.floor(self.x.shape[0] / 10)\n",
    "        \n",
    "    def get_cross_val(self, iteration):\n",
    "        if iteration > 10:\n",
    "            raise ValueError('Crossval Iteration Exceeds 10')\n",
    "        \n",
    "        iteration = iteration - 1\n",
    "        train_array = list(range(0, iteration*self.tenth)) + list(range((iteration+1)*self.tenth, self.x.shape[0]))\n",
    "        test_array = list(range(iteration*self.tenth, (iteration+1)*self.tenth))\n",
    "        \n",
    "        x_train = self.x.iloc[train_array]\n",
    "        y_train = self.y.iloc[train_array]\n",
    "        x_test = self.x.iloc[test_array]\n",
    "        y_test = self.y.iloc[test_array]\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "def build_classifier(parameters):\n",
    "    classifier = Sequential() # This is the ANN object\n",
    "    classifier.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "    \n",
    "    for layer in range(0, parameters['hidden_layers']):\n",
    "        classifier.add(Dense(units=parameters['nodes'], kernel_initializer='uniform', activation='relu')) #Creates first hidden layer\n",
    "        classifier.add(Dropout(rate=0.5))\n",
    "        classifier.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "\n",
    "    classifier.add(Dense(units=3, kernel_initializer='uniform', activation='softmax')) # Output layer. Only 1 ouput category, sigmoid activation to get probability of sureness\n",
    "    # Note: Softmax applies to a dependent variable that has more than 2 categories\n",
    "    # i.e. fMRI categorizations\n",
    "    \n",
    "    classifier.compile(loss='categorical_crossentropy',\n",
    "              optimizer=parameters['optimizer'],\n",
    "              metrics=['accuracy'])\n",
    "    # Notes\n",
    "    # adam is a kind of stochastic gradient descent\n",
    "    # For multivariabel, use categorical cross entropy\n",
    "    # Accuracy is predefined\n",
    "    return classifier\n",
    "# Creates a standard Keras type classifier composed of the defined network for\n",
    "# k-means testing\n",
    "data = data_handler('./atlas.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0707 - acc: 0.5000 - val_loss: 1.0961 - val_acc: 0.5000\n",
      "0\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.1203 - acc: 0.3571 - val_loss: 1.0784 - val_acc: 0.5000\n",
      "1\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.1008 - acc: 0.3571 - val_loss: 1.0815 - val_acc: 0.6667\n",
      "2\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 633s 45s/step - loss: 1.0929 - acc: 0.4286 - val_loss: 1.0957 - val_acc: 0.3333\n",
      "3\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 633s 45s/step - loss: 1.1005 - acc: 0.2143 - val_loss: 1.0926 - val_acc: 0.3333\n",
      "4\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 633s 45s/step - loss: 1.1065 - acc: 0.2857 - val_loss: 1.0941 - val_acc: 0.3333\n",
      "5\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 633s 45s/step - loss: 1.0936 - acc: 0.5000 - val_loss: 1.0745 - val_acc: 0.5000\n",
      "6\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 633s 45s/step - loss: 1.0746 - acc: 0.4286 - val_loss: 1.0755 - val_acc: 0.3333\n",
      "7\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0889 - acc: 0.3571 - val_loss: 1.0932 - val_acc: 0.5000\n",
      "8\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0896 - acc: 0.4286 - val_loss: 1.0975 - val_acc: 0.3333\n",
      "9\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 633s 45s/step - loss: 1.0945 - acc: 0.5000 - val_loss: 1.0919 - val_acc: 0.5000\n",
      "10\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 633s 45s/step - loss: 1.0999 - acc: 0.4286 - val_loss: 1.0815 - val_acc: 0.1667\n",
      "11\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 640s 46s/step - loss: 1.0728 - acc: 0.4286 - val_loss: 1.0820 - val_acc: 0.5000\n",
      "12\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.1158 - acc: 0.4286 - val_loss: 1.0690 - val_acc: 0.8333\n",
      "13\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 45s/step - loss: 1.1320 - acc: 0.2857 - val_loss: 1.0799 - val_acc: 0.5000\n",
      "14\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0874 - acc: 0.6429 - val_loss: 1.0649 - val_acc: 0.6667\n",
      "15\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 638s 46s/step - loss: 1.0593 - acc: 0.5000 - val_loss: 1.1148 - val_acc: 0.0000e+00\n",
      "16\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.1011 - acc: 0.2857 - val_loss: 1.0819 - val_acc: 0.6667\n",
      "17\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0732 - acc: 0.5000 - val_loss: 1.1093 - val_acc: 0.1667\n",
      "18\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.1296 - acc: 0.2143 - val_loss: 1.0691 - val_acc: 0.5000\n",
      "19\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0768 - acc: 0.4286 - val_loss: 1.0868 - val_acc: 0.5000\n",
      "20\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 45s/step - loss: 1.1058 - acc: 0.1429 - val_loss: 1.0886 - val_acc: 0.5000\n",
      "21\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 638s 46s/step - loss: 1.0806 - acc: 0.4286 - val_loss: 1.1184 - val_acc: 0.3333\n",
      "22\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0731 - acc: 0.3571 - val_loss: 1.0807 - val_acc: 0.5000\n",
      "23\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0750 - acc: 0.4286 - val_loss: 1.1026 - val_acc: 0.3333\n",
      "24\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 45s/step - loss: 1.1000 - acc: 0.3571 - val_loss: 1.0996 - val_acc: 0.3333\n",
      "25\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0616 - acc: 0.4286 - val_loss: 1.0903 - val_acc: 0.1667\n",
      "26\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0797 - acc: 0.6429 - val_loss: 1.0535 - val_acc: 0.8333\n",
      "27\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0343 - acc: 0.5000 - val_loss: 1.1012 - val_acc: 0.3333\n",
      "28\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0873 - acc: 0.2857 - val_loss: 1.0519 - val_acc: 0.6667\n",
      "29\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.0735 - acc: 0.6429 - val_loss: 1.0895 - val_acc: 0.5000\n",
      "30\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 637s 46s/step - loss: 1.1306 - acc: 0.1429 - val_loss: 1.0708 - val_acc: 0.5000\n",
      "31\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 641s 46s/step - loss: 1.0711 - acc: 0.3571 - val_loss: 1.0865 - val_acc: 0.3333\n",
      "32\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0748 - acc: 0.5000 - val_loss: 1.0815 - val_acc: 0.5000\n",
      "33\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0660 - acc: 0.5000 - val_loss: 1.0785 - val_acc: 0.5000\n",
      "34\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0239 - acc: 0.6429 - val_loss: 1.0500 - val_acc: 0.8333\n",
      "35\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0863 - acc: 0.2857 - val_loss: 1.1046 - val_acc: 0.0000e+00\n",
      "36\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.1405 - acc: 0.1429 - val_loss: 1.0735 - val_acc: 0.6667\n",
      "37\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "tensorboard = TensorBoard(log_dir=\"../logs/{}\".format(time.time()))\n",
    "\n",
    "for e in range(parameters['epochs']):\n",
    "    X_batch, y_batch = batcher.next_batch(parameters['batch_size'])\n",
    "    classifier.fit(np.array(X_batch), np.array(y_batch), batch_size=parameters['batch_size'], epochs=1,callbacks=[tensorboard], validation_split=0.3)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    i = i + 1\n",
    "    tensorboard = Tensorboard(log_dir=\"../logs/{}\".format(time.time()))\n",
    "    x_train, y_train, x_test, y_test = data.get_cross_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
