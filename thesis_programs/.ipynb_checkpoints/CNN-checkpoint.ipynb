{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv3d_11 (Conv3D)           (None, 18, 22, 18, 50)    66600     \n",
      "_________________________________________________________________\n",
      "max_pooling3d_7 (MaxPooling3 (None, 16, 20, 16, 50)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 16, 20, 16, 50)    200       \n",
      "_________________________________________________________________\n",
      "conv3d_12 (Conv3D)           (None, 12, 16, 12, 50)    312550    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_8 (MaxPooling3 (None, 11, 15, 11, 50)    0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 11, 15, 11, 50)    200       \n",
      "_________________________________________________________________\n",
      "conv3d_13 (Conv3D)           (None, 9, 13, 9, 100)     135100    \n",
      "_________________________________________________________________\n",
      "conv3d_14 (Conv3D)           (None, 7, 11, 7, 100)     270100    \n",
      "_________________________________________________________________\n",
      "conv3d_15 (Conv3D)           (None, 5, 9, 5, 50)       135050    \n",
      "_________________________________________________________________\n",
      "max_pooling3d_9 (MaxPooling3 (None, 4, 8, 4, 50)       0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4, 8, 4, 50)       0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 264)               1689864   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 264)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 264)               69960     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 264)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 264)               69960     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 264)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 795       \n",
      "=================================================================\n",
      "Total params: 2,750,379\n",
      "Trainable params: 2,750,179\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# This is a practice Artificial Neural Network\n",
    "# The problem being solved is based off of a model bank with fake data\n",
    "# The bank has customers that have left for whatever reason\n",
    "# The goal is to find why these customers have left using information such as account balance and gender\n",
    "# The last column of the data states whether or not the customer has left the bank\n",
    "\n",
    "import os # Navigating directories\n",
    "import nibabel as nib # For nii files\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np #Math operations library\n",
    "import matplotlib.pyplot as plt #Visualization library\n",
    "import pandas as pd #Matrix handler\n",
    "\n",
    "import keras # Brings in tensorflow with it\n",
    "from keras.models import Sequential # Used for initialization of ANN\n",
    "from keras.layers import Dense, Conv3D, MaxPooling3D, Flatten, Dropout, BatchNormalization# adds layers to ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier # ability to turn network into a function definition\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler #Methods to change categorical strings to numbers and scaling ability\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # Splits data into training and testing\n",
    "from sklearn.metrics import confusion_matrix # Creates truth table for evaluating results\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "parameters = {'batch_size': 20,#4,\n",
    "              'epochs': 500,\n",
    "              'learning_rate': 0.00001,\n",
    "              'nodes': 264,\n",
    "              'large_kernel': 100,\n",
    "              'small_kernel': 50\n",
    "             } # Creates list of parameters to test to find most successful one\n",
    "parameters['optimizer'] = Adam(parameters['learning_rate'])\n",
    "\n",
    "class build_batcher:\n",
    "    def __init__(self, pickle_loc):\n",
    "        self.df = pd.read_pickle(pickle_loc)\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.tenth = math.floor(self.df.shape[0] / 10)\n",
    "        self.set_cross_val(1)\n",
    "    \n",
    "        self.max = 3800\n",
    "        # The following lines find the max value for normalization, however it takes a while. The max is 3799 for this set\n",
    "        # for y in range(0, len(self.df)):\n",
    "        #     img = nib.load(self.df.iloc[y]['Location'])\n",
    "        #     data = img.get_fdata()\n",
    "        #     local_max = max(data.flatten())\n",
    "        #     if local_max > self.max:\n",
    "        #         self.max = local_max\n",
    "        #         print(self.max)\n",
    "        \n",
    "    def set_cross_val(self, iteration):\n",
    "        if iteration > 10:\n",
    "            raise ValueError('Crossval Iteration Exceeds 10')\n",
    "        \n",
    "        iteration = iteration - 1\n",
    "        self.index_array = list(range(0, iteration*self.tenth)) + list(range((iteration+1)*self.tenth, self.df.shape[0]))\n",
    "        self.test_array = list(range(iteration*self.tenth, (iteration+1)*self.tenth))\n",
    "        self.index = 0\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        fetch_array = list(range(0, batch_size))\n",
    "        fetch_array = [x + self.index for x in fetch_array]\n",
    "        \n",
    "        self.index = self.index + batch_size\n",
    "        if self.index > len(self.index_array) - 1:\n",
    "            self.index = self.index - len(self.index_array) - 1\n",
    "            self.epoch = self.epoch + 1\n",
    "        \n",
    "        copy = list([])\n",
    "        for x in fetch_array:\n",
    "            if x > len(self.index_array) - 1:\n",
    "                x = x - len(self.index_array) - 1\n",
    "            copy.append(x)\n",
    "        \n",
    "        fetch_array = copy\n",
    "        del copy\n",
    "        \n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for pic in fetch_array:\n",
    "            row = self.df.iloc[self.index_array[pic]]\n",
    "            nii_data = nib.load(row['Location'])\n",
    "            img_data = nii_data.get_fdata()\n",
    "            img_data.resize(79, 95, 79, 1)\n",
    "            \n",
    "            x_batch.append(img_data)\n",
    "            y_batch.append([row['Rest'], row['Emote'], row['Solve']])\n",
    "            \n",
    "        x_batch = np.asarray(x_batch)\n",
    "        x_batch = np.true_divide(x_batch, self.max)\n",
    "        y_batch = np.asarray(y_batch)\n",
    "        return(x_batch, y_batch)\n",
    "        \n",
    "        \n",
    "    def get_epoch(self):\n",
    "        return(self.epoch)\n",
    "    \n",
    "    def get_index(self):\n",
    "        return(self.index)\n",
    "    \n",
    "    def test_classifier(self, classifier):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for case in self.test_array:\n",
    "            row = self.df.iloc[case]\n",
    "            nii_data = nib.load(row['Location'])\n",
    "            img_data = nii_data.get_fdata()\n",
    "            img_data.resize(1, 79, 95, 79, 1)\n",
    "            classes = classifier.predict(img_data, batch_size=1)\n",
    "            \n",
    "            total = total + 1\n",
    "            if classes[0] == row['Rest'] & classes[1] == row['Emote'] & classes[2] == row['Solve']:\n",
    "                correct = correct + 1\n",
    "        \n",
    "        return(correct / total)\n",
    "\n",
    "# AlexNet specs: https://medium.com/@smallfishbigsea/a-walk-through-of-alexnet-6cbd137a5637\n",
    "def build_classifier(parameters):\n",
    "    classifier = Sequential() # This is the ANN object\n",
    "    \n",
    "    #First Layer\n",
    "    classifier.add(Conv3D(parameters['small_kernel'], (11, 11, 11), input_shape=(79, 95, 79, 1), strides=(4, 4, 4), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "    #Conv3D(32, (3, 3, 3), input_shape=(79, 95, 79), activation='relu')) # 32 kernels, 3x3; input image is rgb and 64x64\n",
    "    #Note: images are not the same size and will need adjustment prior to training\n",
    "    #Note: docs using Theano backend reference the input shape with the channel number first\n",
    "    classifier.add(MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1))) # Mix a 2x2 square into a 1x1\n",
    "    classifier.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "    #Normalizes output from layer\n",
    "\n",
    "    #Second Layer\n",
    "    classifier.add(Conv3D(parameters['small_kernel'], (5, 5, 5), activation='relu', strides=(1, 1, 1), padding='valid'))\n",
    "    classifier.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1)))\n",
    "    classifier.add(BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001, center=True, scale=True, beta_initializer='zeros', gamma_initializer='ones', moving_mean_initializer='zeros', moving_variance_initializer='ones', beta_regularizer=None, gamma_regularizer=None, beta_constraint=None, gamma_constraint=None))\n",
    "    \n",
    "    #Third Layer\n",
    "    classifier.add(Conv3D(parameters['large_kernel'], (3, 3, 3), activation='relu', strides=(1, 1, 1), padding='valid'))\n",
    "    #Fourth\n",
    "    classifier.add(Conv3D(parameters['large_kernel'], (3, 3, 3), activation='relu', strides=(1, 1, 1), padding='valid'))\n",
    "    #Fifth\n",
    "    classifier.add(Conv3D(parameters['small_kernel'], (3, 3, 3), activation='relu', strides=(1, 1, 1), padding='valid'))\n",
    "    classifier.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1)))\n",
    "    classifier.add(Dropout(rate=0.5))\n",
    "    classifier.add(Flatten())\n",
    "    \n",
    "    #FC6\n",
    "    classifier.add(Dense(units=parameters['nodes'], kernel_initializer='uniform', activation='relu')) #Creates first hidden layer\n",
    "    classifier.add(Dropout(rate=0.5))\n",
    "    \n",
    "    classifier.add(Dense(units=parameters['nodes'], kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dropout(rate=0.5))\n",
    "    classifier.add(Dense(units=parameters['nodes'], kernel_initializer='uniform', activation='relu'))\n",
    "    classifier.add(Dropout(rate=0.5))\n",
    "    \n",
    "    \n",
    "    classifier.add(Dense(units=3, kernel_initializer='uniform', activation='softmax')) # Output layer. Only 1 ouput category, sigmoid activation to get probability of sureness\n",
    "    \n",
    "\n",
    "    # Note: Softmax applies to a dependent variable that has more than 2 categories\n",
    "    # i.e. fMRI categorizations\n",
    "    classifier.compile(loss='categorical_crossentropy',\n",
    "              optimizer=parameters['optimizer'],\n",
    "              metrics=['accuracy'])\n",
    "    # Notes\n",
    "    # adam is a kind of stochastic gradient descent\n",
    "    # For multivariabel, use categorical cross entropy\n",
    "    # Accuracy is predefined\n",
    "    return classifier\n",
    "# Creates a standard Keras type classifier composed of the defined network for\n",
    "# k-means testing\n",
    "classifier = build_classifier(parameters)\n",
    "plot_model(classifier, to_file='classifier.png')\n",
    "print(classifier.summary())\n",
    "batcher = build_batcher('/media/jasondent/My Passport/PNC cohort 200/contents.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TensorBoard' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44f8af5d01b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtensorboard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../logs/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TensorBoard' is not defined"
     ]
    }
   ],
   "source": [
    "# model.train_on_batch(x_batch, y_batch)\n",
    "# classes = model.predict(x_test, batch_size=128)\n",
    "\n",
    "epoch = 0\n",
    "tensorboard = TensorBoard(log_dir=\"../logs/{}\".format(time.time()))\n",
    "while(epoch < parameters['epochs']):\n",
    "    x_batch, y_batch = batcher.next_batch(parameters['batch_size'])\n",
    "    loss = classifier.train_on_batch(x_batch, y_batch)\n",
    "    prediction = classifier.predict(x_batch, batch_size=parameters['batch_size'])\n",
    "    print(y_batch)\n",
    "    print(prediction)\n",
    "    print('epoch:\\t' + str(epoch + 1) + '\\tindex:\\t' + str(batcher.get_index()))\n",
    "    if epoch != batcher.get_epoch():\n",
    "        epoch = batcher.get_epoch()\n",
    "# classifier.train_on_batch(data, np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.0707 - acc: 0.5000 - val_loss: 1.0961 - val_acc: 0.5000\n",
      "0\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.1203 - acc: 0.3571 - val_loss: 1.0784 - val_acc: 0.5000\n",
      "1\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n",
      "14/14 [==============================] - 632s 45s/step - loss: 1.1008 - acc: 0.3571 - val_loss: 1.0815 - val_acc: 0.6667\n",
      "2\n",
      "Train on 14 samples, validate on 6 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "epoch = 0\n",
    "tensorboard = TensorBoard(log_dir=\"../logs/{}\".format(time.time()))\n",
    "\n",
    "for e in range(parameters['epochs']):\n",
    "    X_batch, y_batch = batcher.next_batch(parameters['batch_size'])\n",
    "    classifier.fit(np.array(X_batch), np.array(y_batch), batch_size=parameters['batch_size'], epochs=1,callbacks=[tensorboard], validation_split=0.3)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2]\n"
     ]
    }
   ],
   "source": [
    "# https://docs.google.com/document/d/1PfHHsffrxab0_CGUpSCKH3WwYeCiiCmDORiq-jO_OC0/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array(((1,2), (3,4)))\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
