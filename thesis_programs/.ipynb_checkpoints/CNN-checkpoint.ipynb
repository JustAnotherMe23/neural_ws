{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This is a practice Artificial Neural Network\n",
    "# The problem being solved is based off of a model bank with fake data\n",
    "# The bank has customers that have left for whatever reason\n",
    "# The goal is to find why these customers have left using information such as account balance and gender\n",
    "# The last column of the data states whether or not the customer has left the bank\n",
    "\n",
    "import os # Navigating directories\n",
    "import nibabel as nib # For nii files\n",
    "import math\n",
    "\n",
    "import numpy as np #Math operations library\n",
    "import matplotlib.pyplot as plt #Visualization library\n",
    "import pandas as pd #Matrix handler\n",
    "\n",
    "import keras # Brings in tensorflow with it\n",
    "from keras.models import Sequential # Used for initialization of ANN\n",
    "from keras.layers import Dense, Conv3D, MaxPooling3D, Flatten # adds layers to ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier # ability to turn network into a function definition\n",
    "from keras.layers import Dropout\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler #Methods to change categorical strings to numbers and scaling ability\n",
    "from sklearn.model_selection import train_test_split # Splits data into training and testing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix # Creates truth table for evaluating results\n",
    "\n",
    "parameters = {'batch_size': 1,#4,\n",
    "              'epochs': 500,\n",
    "              'optimizer': 'rmsprop',#'adam'],\n",
    "              'nodes': 6,\n",
    "              'hidden_layers': 1,\n",
    "              'num_features': 20,\n",
    "             } # Creates list of parameters to test to find most successful one\n",
    "\n",
    "class build_batcher:\n",
    "    def __init__(self, pickle_loc):\n",
    "        self.df = pd.read_pickle(pickle_loc)\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.tenth = math.floor(self.df.shape[0] / 10)\n",
    "        self.set_cross_val(1)\n",
    "    \n",
    "        self.max = 3800\n",
    "        # The following lines find the max value for normalization, however it takes a while. The max is 3799 for this set\n",
    "        # for y in range(0, len(self.df)):\n",
    "        #     img = nib.load(self.df.iloc[y]['Location'])\n",
    "        #     data = img.get_fdata()\n",
    "        #     local_max = max(data.flatten())\n",
    "        #     if local_max > self.max:\n",
    "        #         self.max = local_max\n",
    "        #         print(self.max)\n",
    "        \n",
    "    def set_cross_val(self, iteration):\n",
    "        if iteration > 10:\n",
    "            raise ValueError('Crossval Iteration Exceeds 10')\n",
    "        \n",
    "        iteration = iteration - 1\n",
    "        self.index_array = list(range(0, iteration*self.tenth)) + list(range((iteration+1)*self.tenth, self.df.shape[0]))\n",
    "        self.test_array = list(range(iteration*self.tenth, (iteration+1)*self.tenth))\n",
    "        self.index = 0\n",
    "        self.epoch = 0\n",
    "        \n",
    "    def next_batch(self, batch_size):\n",
    "        fetch_array = list(range(0, batch_size))\n",
    "        fetch_array = [x + self.index for x in fetch_array]\n",
    "        \n",
    "        self.index = self.index + batch_size\n",
    "        if self.index > len(self.index_array) - 1:\n",
    "            self.index = self.index - len(self.index_array) - 1\n",
    "            self.epoch = self.epoch + 1\n",
    "        \n",
    "        copy = list([])\n",
    "        for x in fetch_array:\n",
    "            if x > len(self.index_array) - 1:\n",
    "                x = x - len(self.index_array) - 1\n",
    "            copy.append(x)\n",
    "        \n",
    "        fetch_array = copy\n",
    "        del copy\n",
    "        \n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "        for pic in fetch_array:\n",
    "            row = self.df.iloc[self.index_array[pic]]\n",
    "            nii_data = nib.load(row['Location'])\n",
    "            img_data = nii_data.get_fdata()\n",
    "            img_data.resize(79, 95, 79, 1)\n",
    "            \n",
    "            x_batch.append(img_data)\n",
    "            y_batch.append([row['Rest'], row['Emote'], row['Solve']])\n",
    "            \n",
    "        x_batch = np.asarray(x_batch)\n",
    "        x_batch = np.true_divide(x_batch, self.max)\n",
    "        y_batch = np.asarray(y_batch)\n",
    "        return(x_batch, y_batch)\n",
    "        \n",
    "        \n",
    "    def get_epoch(self):\n",
    "        return(self.epoch)\n",
    "    \n",
    "    def get_index(self):\n",
    "        return(self.index)\n",
    "    \n",
    "    def test_classifier(self, classifier):\n",
    "        total = 0\n",
    "        correct = 0\n",
    "        for case in self.test_array:\n",
    "            row = self.df.iloc[case]\n",
    "            nii_data = nib.load(row['Location'])\n",
    "            img_data = nii_data.get_fdata()\n",
    "            img_data.resize(1, 79, 95, 79, 1)\n",
    "            classes = classifier.predict(img_data, batch_size=1)\n",
    "            \n",
    "            total = total + 1\n",
    "            if classes[0] == row['Rest'] & classes[1] == row['Emote'] & classes[2] == row['Solve']:\n",
    "                correct = correct + 1\n",
    "        \n",
    "        return(correct / total)\n",
    "\n",
    "def build_classifier(nodes, hidden_layers, num_features):\n",
    "    classifier = Sequential() # This is the ANN object\n",
    "    classifier.add(Conv3D(32, (3, 3, 3), input_shape=(79, 95, 79, 1), strides=(1, 1, 1), padding='valid', data_format=None, dilation_rate=(1, 1, 1), activation='relu', use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None))\n",
    "    #Conv3D(32, (3, 3, 3), input_shape=(79, 95, 79), activation='relu')) # 32 kernels, 3x3; input image is rgb and 64x64\n",
    "    #Note: images are not the same size and will need adjustment prior to training\n",
    "    #Note: docs using Theano backend reference the input shape with the channel number first\n",
    "    classifier.add(MaxPooling3D(pool_size=(2, 2, 2))) # Mix a 4x4 square into a 1x1\n",
    "    classifier.add(Conv3D(32, (3, 3, 3), activation='relu'))\n",
    "    classifier.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "    classifier.add(Flatten())\n",
    "    \n",
    "    classifier.add(Dense(input_dim=num_features, units=nodes, kernel_initializer='uniform', activation='relu')) #Creates first hidden layer\n",
    "    classifier.add(Dropout(rate=0.1))\n",
    "    \n",
    "    for i in range(0, hidden_layers):\n",
    "        classifier.add(Dense(units=nodes, kernel_initializer='uniform', activation='relu')) # Second layer. Input dim is known from previous layer\n",
    "        classifier.add(Dropout(rate=0.1))\n",
    "    \n",
    "    classifier.add(Dense(units=3, kernel_initializer='uniform', activation='softmax')) # Output layer. Only 1 ouput category, sigmoid activation to get probability of sureness\n",
    "    classifier.add(Dropout(rate=0.1))\n",
    "\n",
    "    # Note: Softmax applies to a dependent variable that has more than 2 categories\n",
    "    # i.e. fMRI categorizations\n",
    "\n",
    "    classifier.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "    # Notes\n",
    "    # adam is a kind of stochastic gradient descent\n",
    "    # For multivariabel, use categorical cross entropy\n",
    "    # Accuracy is predefined\n",
    "    return classifier\n",
    "# Creates a standard Keras type classifier composed of the defined network for\n",
    "# k-means testing\n",
    "\n",
    "classifier = build_classifier(parameters['nodes'], parameters['hidden_layers'], parameters['num_features'])\n",
    "batcher = build_batcher('/media/jasondent/My Passport/PNC cohort 200/contents.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]]\n",
      "[[0. 0. 0.]]\n",
      "epoch:\t1\tindex:\t31\n",
      "[[0 0 1]]\n",
      "[[0. 0. 1.]]\n",
      "epoch:\t1\tindex:\t32\n",
      "[[1 0 0]]\n",
      "[[0. 0. 0.]]\n",
      "epoch:\t1\tindex:\t33\n",
      "[[0 0 1]]\n",
      "[[0. 0. 0.]]\n",
      "epoch:\t1\tindex:\t34\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5407126cf46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model.train_on_batch(x_batch, y_batch)\n",
    "# classes = model.predict(x_test, batch_size=128)\n",
    "\n",
    "epoch = 0\n",
    "while(epoch < parameters['epochs']):\n",
    "    x_batch, y_batch = batcher.next_batch(parameters['batch_size'])\n",
    "    loss = classifier.train_on_batch(x_batch, y_batch)\n",
    "    prediction = classifier.predict(x_batch, batch_size=parameters['batch_size'])\n",
    "    print(y_batch)\n",
    "    print(prediction)\n",
    "    print('epoch:\\t' + str(epoch + 1) + '\\tindex:\\t' + str(batcher.get_index()))\n",
    "    if epoch != batcher.get_epoch():\n",
    "        epoch = batcher.get_epoch()\n",
    "# classifier.train_on_batch(data, np.array([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method build_batcher.get_epoch of <__main__.build_batcher object at 0x7fbc08591ba8>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
